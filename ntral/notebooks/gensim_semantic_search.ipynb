{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim text retrieval semantic engine\n",
    "\n",
    "* Gensim text retrieval semantic engine with Latent Semantic Indexing (LSA in TR).\n",
    "* Dataset is https://www.kaggle.com/rmisra/news-category-dataset with 202372 entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/News_Category_Dataset_v2.json\"\n",
    "DATA_LEN = 202372"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "def tokenize(text):\n",
    "    return [token for token in gensim.utils.simple_preprocess(text) if token not in STOPWORDS]\n",
    "\n",
    "def iter_news(file):\n",
    "    for line in open(file):\n",
    "        line = json.loads(line)['headline'] + json.loads(line)['short_description']\n",
    "        tokens = tokenize(line)\n",
    "        yield line, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "import json\n",
    "\n",
    "class NewsCorpus(object):\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.dictionary = Dictionary()\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for line in open(self.path):\n",
    "            \n",
    "            line = json.loads(line)['headline'] + json.loads(line)['short_description']\n",
    "            tokens = tokenize(line)\n",
    "            # yield self.dictionary.doc2bow(line['headline'].lower().split())\n",
    "            # yield self.dictionary.doc2bow(json.loads(line)['headline'].lower().split())\n",
    "            yield line, tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.6 s\n",
      "Dictionary(168877 unique tokens: ['america', 'children', 'day', 'husband', 'killed']...)\n"
     ]
    }
   ],
   "source": [
    "# stream just tokens\n",
    "doc_stream = (tokens for _, tokens in iter_news(DATA_PATH))\n",
    "\n",
    "# build dict\n",
    "%time id2word_news = gensim.corpora.Dictionary(doc_stream)\n",
    "print(id2word_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(14366 unique tokens: ['america', 'children', 'day', 'husband', 'killed']...)\n"
     ]
    }
   ],
   "source": [
    "# ignore words that appear in less than 20 documents or more than 10% documents\n",
    "id2word_news.filter_extremes(no_below=20, no_above=0.1)\n",
    "print(id2word_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsCorpus():\n",
    "    \n",
    "    def __init__(self, file, dictionary):\n",
    "        self.file = file\n",
    "        self.dict = dictionary\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.titles = []\n",
    "        for title, tokens in iter_news(self.file):\n",
    "            self.titles.append(title)\n",
    "            yield self.dict.doc2bow(tokens)\n",
    "        \n",
    "            \n",
    "# create a stream of bag-of-words vectors\n",
    "news_corpus = NewsCorpus(DATA_PATH, id2word_news)\n",
    "vector = next(iter(news_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18.7 s\n"
     ]
    }
   ],
   "source": [
    "# store corpus\n",
    "%time gensim.corpora.MmCorpus.serialize('../data/news_bow.mm', news_corpus)\n",
    "\n",
    "# store dictionary\n",
    "id2word_news.save('../data/news.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(200853 documents, 14366 features, 2548042 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "# load dictionary\n",
    "id2word_news = gensim.corpora.Dictionary.load('../data/news.dict')\n",
    "\n",
    "# load corpus\n",
    "mm_corpus = gensim.corpora.MmCorpus('../data/news_bow.mm')\n",
    "print(mm_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 46.6 s\n"
     ]
    }
   ],
   "source": [
    "%time lsi = gensim.models.lsimodel.LsiModel(corpus=mm_corpus, id2word=id2word_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi.save('../data/lsi_news.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi = lsi.load('../data/lsi_news.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 37.7 s\n"
     ]
    }
   ],
   "source": [
    "# build the index\n",
    "from gensim import similarities\n",
    "%time index = similarities.Similarity('../data/', lsi[mm_corpus], num_features = lsi.num_topics)\n",
    "#%time index = similarities.Similarity(lsi[mm_corpus])\n",
    "index.save('../data/lsi_news.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "lsi = gensim.models.lsimodel.LsiModel.load('../data/lsi_news.model')\n",
    "index = gensim.similarities.Similarity.load('../data/lsi_news.index')\n",
    "dictionary = gensim.corpora.Dictionary.load('../data/news.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform doc into lsi vector space (we need the model for this)\n",
    "doc = \"disaster\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "\n",
    "# query doc (we need the index for this)\n",
    "sims = index[vec_lsi]\n",
    "\n",
    "# sort by similarity\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(19424, 0.7097067), (17114, 0.7094732), (17451, 0.68108934), (78505, 0.6792595), (45511, 0.6750591), (46051, 0.6609052), (50149, 0.6568615), (175384, 0.6479787), (17197, 0.647551), (78660, 0.64356035)]\n"
     ]
    }
   ],
   "source": [
    "# print most similar docs\n",
    "print(sims[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Struggling Haiti Scrambles To Prepare For Hurricane Irma\n",
      "\tBarreling through the Caribbean, the â€œextremely dangerousâ€ core of Irma was predicted to strike northern Haiti.\n",
      "Hail The Unsung Heroes Of Hurricane Harvey\n",
      "\tI am frustrated by the lack of understanding that undermines the efforts of many dedicated staff and volunteers in the nonprofit\n",
      "Irma Intensifies Into Category 3 Hurricane\n",
      "\tThe hurricane is over the eastern Atlantic and headed toward the Caribbean.\n",
      "Chronicling A Forgotten Disaster: Hurricane Matthew, 10 Months Later\n",
      "\tMatthew was destined to be a forgotten disaster, in the shadow of Haitiâ€™s 2010 earthquake and overshadowed by the U.S. elections.\n",
      "U.S. Suspends Deportations Of Haitians After Hurricane Matthew\n",
      "\tA brief reprieve.\n",
      "'Evil Sodomites' Now Being Blamed For Hurricane Matthew\n",
      "\tðŸ™ƒ\n",
      "Hurricane Katrina Survivors Relive Familiar Nightmare In Baton Rouge\n",
      "\tThe recent flooding in southern Louisiana has brought back horrific memories for some residents.\n",
      "Fate Of Cargo Ship Unknown As Hurricane Joaquin Batters Bahamas\n",
      "\tThe fate of more than 30 crew aboard a cargo ship missing off the Bahamas in heavy seas whipped up by Hurricane Joaquin was\n",
      "Hurricane Joaquin Strengthens To Category 4, Batters Bahamas\n",
      "\tThe storm is expected to move near or over portions of the central Bahamas overnight.\n",
      "How Fujiwhara Effect Will Toss Hurricane Sandy Into U.S.\n",
      "\tA vortex is a flow pattern in a fluid that has rotation about a center:Â water spiraling down the bathtub drain, the swirling\n"
     ]
    }
   ],
   "source": [
    "# get documents. gensim leaves the metadata of the corpus element to the user.\n",
    "# this is helpful for database access of the document, but for text files we need\n",
    "# to have a dict in memory or just go line by line of the file.\n",
    "\n",
    "import json\n",
    "ids = [i[0] for i in sims[:10]]\n",
    "\n",
    "fp = open(DATA_PATH)\n",
    "for i, line in enumerate(fp):\n",
    "    if i in ids:\n",
    "        line = json.loads(line)\n",
    "        line['headline']\n",
    "        line['short_description']\n",
    "        print(\"{}\\n\\t{}\".format(line['headline'], line['short_description']))\n",
    "        \n",
    "    if i > max(ids):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
